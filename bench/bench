#!/usr/bin/env python3
# Basic usage:
#   ./bench.py binary [test.js ...]
#
# Conceptually, runs './binary test.js' on each test multiple times,
# manages quirks of different engines, default set of benchmarks,
# parses errors and test results.
#
# SPDX-FileCopyrightText: 2025 Ivan Krasilnikov
# SPDX-License-Identifier: MIT

from __future__ import annotations

import argparse
import datetime
import json
import math
import os
import random
import re
import shlex
import shutil
import signal
import subprocess
import sys
import tempfile
import threading
import time

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable

START_TIME = datetime.datetime.now(datetime.timezone.utc).strftime('%Y-%m-%d %H:%M:%S.%f %Z')
PERIODIC_SAVE_SECONDS = 10

V8_V7_TESTS = [
   'richards.js',
   'deltablue.js',
   'crypto.js',
   'raytrace.js',
   'earley-boyer.js',
   'regexp.js',
   'splay.js',
   'navier-stokes.js',
]

OCTANE_TESTS = [
   'richards.js',
   'deltablue.js',
   'crypto.js',
   'raytrace.js',
   'earley-boyer.js',
   'regexp.js',
   'splay.js',
   'navier-stokes.js',
   'pdfjs.js',
   'mandreel.js',
   'gbemu.js',
   'code-load.js',
   'box2d.js',
   'zlib.js',
   'typescript.js',
]

PRINT_PF = '''
  if (typeof print == "undefined" && typeof console != "undefined") {
    if (typeof globalThis == "object") globalThis.print = console.log;
    else if (typeof this == "object") this.print = console.log;
    if (typeof print == "undefined") print = console.log;
  }
  if (typeof console == "undefined") {
    if (typeof globalThis == "object") globalThis.console = new Object();
    else if (typeof this == "object") this.console = new Object();
    if (typeof console == "undefined") console = new Object();
  }
  if (typeof console.log == "undefined" && typeof print != "undefined") {
    console.log = print;
  }
'''

ES5_FOR_ES3_PF = '''
  if (typeof Array.prototype.indexOf === "undefined") {
    Array.prototype.indexOf = function(x, i) {
      for (i = i || 0; i < this.length; i++) if (this[i] === x) return i;
      return -1;
    };
  }
  if (typeof Array.prototype.map === "undefined") {
    Array.prototype.map = function(f, t) {
      for (var i = 0, res = []; i < this.length; i++)
        if (i in this) res[i] = f.call(t, this[i], i, this);
      return res;
    };
  }
  if (typeof Object.create === "undefined") {
    Object.create = function(p) { function F() {}; F.prototype = p; return new F(); };
  }
  if (typeof Object.defineProperty === "undefined") {
    Object.defineProperty = function(obj, prop, desc) {
      if (desc.hasOwnProperty('value')) {
        obj[prop] = desc.value;
      } else if (desc.hasOwnProperty('get')) {
        obj[prop] = desc.get.call(obj);
      }
      return obj;
    };
  }
'''

# For engines that run in strict mode by default
STRICT_PF = '''
  var alert = print;
  var setupEngine, nValue, eValue, dValue, pValue, qValue, dmp1Value, dmq1Value, coeffValue;  // crypto.js
  var result;  // earley-boyer.js
  var Mandreel_timeouts, Mandreel_XMLHttpRequest, Mandreel_document, Mandreel_window;  // mandreel.js
'''

@dataclass
class MemTest:
    """In-memory test case, a single self-contained script."""

    basename: str
    script: str

def load_test(path: str | Path, drop_loads=False) -> MemTest:
    """Read test case into memory, recursively resolving load() calls."""

    path = Path(path)
    basename = path.name
    lines = []

    for line in open(path):
        m = re.match(r'''^load\((?:base_dir \+ )?('[^']+'|"[^"\\]+")\) *;$''', line.strip());
        if not m:
            lines.append(line)
            continue

        lines.append('// BEGIN ' + line.strip() + '\n')
        if drop_loads:
            continue

        loaded = load_test(path.parent / m[1][1:-1])
        lines.append(loaded.script.strip() + '\n')
        lines.append('// END ' + line.strip() + '\n\n')

    script = ''.join(lines).strip() + '\n'
    return MemTest(basename=basename, script=script)

type TestTransform = Callable[[MemTest], str | MemTest]

class OctaneHarness:
    def __init__(self):
        self._base = None
        self._tail = None

    def __call__(self, test: MemTest) -> MemTest:
        if test.basename not in OCTANE_TESTS:
            return test

        if 'function Benchmark(' in test.script:
            return test

        if self._base is None:
            self._base = open('octane/base.js').read().strip() + '\n'

        if self._tail is None:
            self._tail = open('octane/run.js').read().strip() + '\n'
            self._tail = re.sub(r'load\([^)]+\);', '', self._tail)

        script = self._base + test.script + self._tail
        return MemTest(basename=test.basename, script=script)

class OctalTransform:
    """Fix octal char sequences in earley-boyer.js, "\000" -> "\x00" etc."""

    def __call__(self, test: MemTest) -> str:
        return re.sub(r'"\\[01]([0-9][0-9])"', self._replace, test.script)

    def _replace(self, match: re.Match) -> str:
        return '"\\x%.2X"' % ord(eval(match[0])[0])

class HexTransform:
    """Replace hex literals by decimal."""

    def __call__(self, test: MemTest) -> str:
        return re.sub(r'''(0x[0-9a-fA-F]+|"(?:[^"\n]|\")+"|'(?:'[^'\n]|\')+')''', self._replace, test.script)

    def _replace(self, match: re.Match) -> str:
        if match[0].startswith('0x'):
            return str(eval(match[0]))
        return match[0]

#lambda test: re.sub(r'performance.now = \(function\(\) {[^}]+}\)\(\);',
#                    '', test.script, re.MULTILINE),

class PolyfillTransform:
    def __init__(self, snippets: list[str]):
        lines = []
        for snippet in snippets:
            for line in snippet.strip().split('\n'):
                lines.append(line.strip())
        self.code = '\n'.join(lines) + '\n'

    def __call__(self, test: MemTest) -> str:
        #if 'es5.strict' in test.basename and "'use strict'" in test.script:
        #    return "'use strict';\n" + self.code + test.script
        return self.code + test.script

class Engine:
    path: Path
    flags: list[str] | None
    metadata: dict[str, Any]
    config: Config | None
    output_path: Path | None
    runs: list[Run]
    current_thread: threading.Thread | None
    current_run: Run | None
    bench_json: dict[str, Any] | None

    def __init__(self, path_and_flags: str):
        self.path = Path(path_and_flags)
        self.flags = None
        self.metadata = {}
        self.config = None
        self.output_path = None
        self.runs = []
        self.current_thread = None
        self.current_run = None
        self.bench_json = None

        if ' ' in path_and_flags and not self.path.exists():
            args = shlex.split(path_and_flags)
            assert len(args) >= 2
            self.path = Path(args[0])
            self.flags = args[1:]

        # allow benchmarking binaries in PATH, e.g. node
        if not os.path.exists(self.path) and '/' not in str(self.path):
            which_path = shutil.which(self.path)
            if which_path:
                self.path = Path(which_path)

        if not os.path.exists(self.path):
            sys.exit(f"{self.path} doesn't exist")

        self.path = Path(os.path.abspath(str(self.path)))

        json_path = self.path.parent / f'{self.path.name}.json'
        if json_path.exists():
            self.metadata = json.load(json_path.open())

        self.bench_json = {
            'binary': self.path.name,
            'flags': self.flags,
            'metadata': self.metadata,
            'time': START_TIME,
            'benchmarks': {},
        }

    def kill(self):
        """Kill active subprocess if running and wait for thread."""
        if self.current_run and self.current_run.proc and self.current_run.proc.poll() is None:
            try:
                os.killpg(os.getpgid(self.current_run.proc.pid), signal.SIGKILL)
                self.current_run.proc.wait()
            except:
                pass
        if self.current_thread:
            self.current_thread.join(timeout=1.0)

    def __str__(self):
        return f'Engine({repr(self.__dict__)})'

    def add_run(self, run: Run):
        """Append a Run to runs list and add its metrics to bench_json."""
        self.runs.append(run)

        if self.bench_json is None:
            return

        if 'benchmarks' not in self.bench_json:
            self.bench_json['benchmarks'] = {}

        # Append time if --append flag was used to help detect non-paired runs
        if not self.bench_json['time'].endswith(START_TIME):
            self.bench_json['time'] += ', ' + START_TIME

        if not self.bench_json['benchmarks']:
            self.bench_json['flags'] = run.flags
        else:
            existing = self.bench_json.get('flags') or []
            assert existing == run.flags, f"flags mismatch: {existing} vs {run.flags}"

        for key in run.scores.keys():
            if key not in self.bench_json['benchmarks']:
                self.bench_json['benchmarks'][key] = {
                    'score': [], 'error': '', 'user': [], 'sys': [], 'real': [], 'rss_mb': []
                }

            d = self.bench_json['benchmarks'][key]
            if run.errors:
                d['error'] = run.errors[0]
            if run.scores[key] is not None:
                d.setdefault('score', []).append(run.scores[key])
            if run.user_time is not None:
                d.setdefault('user', []).append(run.user_time)
            if run.sys_time is not None:
                d.setdefault('sys', []).append(run.sys_time)
            if run.real_time is not None:
                d.setdefault('real', []).append(run.real_time)
            if run.max_rss_kb is not None:
                d.setdefault('rss_mb', []).append(round(run.max_rss_kb / 1024.0, 2))

    def bench_json_str(self) -> str:
        """Serialize bench_json to a formatted JSON string."""
        if self.bench_json is None:
            return '{}'

        # Work on a copy to not modify the original
        res = dict(self.bench_json)

        # Remove empty top-level fields
        for key in ['flags', 'metadata']:
            if not res.get(key):
                res.pop(key, None)

        # Clean up empty arrays and error fields in benchmarks
        res['benchmarks'] = {}
        for key, d in self.bench_json['benchmarks'].items():
            d = dict(d)
            if d.get('error'):
                if 'score' in d:
                    del d['score']
            else:
                if 'error' in d:
                    del d['error']
            for k in ['user', 'sys', 'real', 'rss_mb']:
                if k in d and (d[k] is None or len(d[k]) == 0):
                    del d[k]
            res['benchmarks'][key] = d

        # indent except arrays
        s = json.dumps(res, indent=2)
        s = re.sub(r'(?<=": )(\[[^\[\]]+\])', lambda m: json.dumps(json.loads(m[1])), s)
        return s

@dataclass
class Run:
    temp: dict[str, Path]  # Temp paths
    binary_path: Path
    flags: list[str]
    metadata: dict[str, Any]
    test: MemTest
    test_basename: str
    args: argparse.Namespace
    timeout: float | None = None
    command: str = ''  # bash command
    output: str = ''   # stdout+stderr combined
    errors: list[str] = field(default_factory=list)
    # Result of subprocess.Popen() with self.command
    proc: subprocess.Popen[bytes] | None = None
    # exit_code..max_rss_kb: as measured by /usr/bin/time
    # Note: exit_code captures exit code of javascript shell itself,
    # vs proc.returncode - exit code of bash command/pipeline
    exit_code: int | None = None
    exit_signal: int | None = None
    user_time: float | None = None  # seconds
    real_time: float | None = None
    sys_time: float | None = None
    max_rss_kb: int | None = None
    scores: dict[str, int | float | None] = field(default_factory=dict)

    def to_dict(self):
        res = {}
        for k, v in self.__dict__.items():
            if k in ['proc', 'args', 'temp']:
                continue
            elif hasattr(v, 'to_dict'):
                res[k] = v.to_dict()
            else:
                res[k] = v
        return res

ERROR_LINE_REGEX_I = "(?i)(?:error|panic|exception|uncaught|mismatch|failed|invalid|incorrect|unsupported|cannot|can't)"
ERROR_LINE_REGEX = "(?:error|panic|exception|uncaught|mismatch|failed|invalid|incorrect|unsupported|cannot|can't)"

DEFAULT_TIMEOUT = 1800

class Config:
    # Flags to pass to binary (before script path). binary.flags takes precedence.
    flags: list[str]
    transforms: list[TestTransform]
    polyfills: list[str]
    # Pattern for error lines
    error_lines_re: re.Pattern
    # Pattern for mere warning lines, takes precedence over error_lines_re
    warn_lines_re: re.Pattern | None
    # Pattern for lines to immediately filter out during the run, via egrep
    filter_lines_re: str | None
    timestamp_output: bool
    timeout: float
    # test basename => timeout
    timeout_for_test: dict[str, float]
    ignore_errors: bool
    benchmark_suite: list[str] | None

    def __init__(
            self,
            flags: list[str] = [],
            polyfills: list[str] = [PRINT_PF],
            transforms: list[TestTransform] = [],
            error_lines_re: str = ERROR_LINE_REGEX_I,
            warn_lines_re: str | None = None,
            filter_lines_re: str | None = None,
            timestamp_output: bool = False,
            timeout: float = DEFAULT_TIMEOUT,
            timeout_for_test: dict[str, float] = {},
            ignore_errors: bool = False,
            benchmark_suite: list[str] | None = None,
        ):
        self.flags = list(flags)
        self.transforms = list(transforms)
        if polyfills:
            self.transforms.append(PolyfillTransform(polyfills))
        self.error_lines_re = re.compile(error_lines_re)
        self.warn_lines_re = re.compile(warn_lines_re) if warn_lines_re else None
        self.filter_lines_re = filter_lines_re
        self.timestamp_output = timestamp_output
        self.timeout = timeout
        self.timeout_for_test = timeout_for_test
        self.ignore_errors = ignore_errors
        self.benchmark_suite = benchmark_suite

    def benchmark_run(self, engine: Engine, test: MemTest, args: argparse.Namespace) -> Run:
        temp_dir = Path(tempfile.mkdtemp(prefix=f'{engine.path.name}-{test.basename.removesuffix(".js")}-'))

        run = Run(
            temp = {
                'dir': temp_dir,
                'output': temp_dir / 'output',
                'time': temp_dir / 'time',
                'script': temp_dir / test.basename,
            },
            binary_path=engine.path,
            flags=(self.flags if engine.flags is None else engine.flags),
            metadata=engine.metadata,
            test=test,
            test_basename=test.basename,
            args=args,
        )

        engine.current_run = run

        if run.args.timeout:
            run.timeout = run.args.timeout
        else:
            run.timeout = self.timeout_for_test.get(
                os.path.basename(run.test_basename), self.timeout)

        self.transform_test(run)
        self.save_script(run)
        self.build_command(run)
        self.run_command(run)

        if run.temp['output'].exists():
            run.output = run.temp['output'].open().read()

        self.parse_time_output(run)
        self.extract_benchmark_scores(run)
        self.check_errors(run)

        if run.errors or run.args.keep:
            with open(temp_dir / 'run.json', 'w') as fp:
                json.dump(run.__dict__, indent=2, fp=fp, default=repr)
        else:
            shutil.rmtree(temp_dir)

        engine.current_run = None
        return run

    def transform_test(self, run: Run):
        test = run.test
        for f in self.transforms:
            res = f(test)
            if type(res) is MemTest:
                test = res
            else:
                assert type(res) is str
                test = MemTest(basename=test.basename, script=res)
        run.test = test

    def save_script(self, run: Run):
        with open(run.temp['script'], 'w') as fp:
            fp.write(run.test.script)

    def build_command(self, run: Run):
        run.command = shlex.join(['cd', run.temp['dir'].as_posix()])
        run.command += '; ' + shlex.join(
            ['stdbuf', '-oL', '-eL'] +
            ['/usr/bin/time', '-v', '-o', 'time'] +
            [run.binary_path.as_posix()] +
            run.flags +
            [run.temp['script'].name]
        ) + ' 2>&1'
        if self.timestamp_output:
            # Prefix output lines with time relative to start, n.nnnnnn
            run.command += ' | ts -s %.s'
        if self.filter_lines_re:
            run.command += ' | ' + shlex.join(['egrep', '-v', '-e', self.filter_lines_re])
        run.command += ' | tee output'

    def run_command(self, run: Run):
        if run.args.verbose:
            print(f'> {run.command}', flush=True)
        try:
            run.proc = subprocess.Popen(
                ['/bin/bash', '-e', '-o', 'pipefail', '-c', run.command],
                start_new_session=True,
            )
            run.proc.wait(timeout=run.timeout)
        except subprocess.TimeoutExpired:
            run.errors.append('Timeout (>%.0fs)' % (run.timeout or 0))
        finally:
            # Kill whole process group, not just directly launched process
            if run.proc is not None and run.proc.poll() is None:
                os.killpg(os.getpgid(run.proc.pid), signal.SIGKILL)
                run.proc.wait()

    def parse_time_output(self, run: Run):
        """Parse the output of /usr/bin/time -v"""

        assert run.temp['time'].exists()

        keymap = {
            'User time (seconds)': 'user_time',
            'System time (seconds)': 'sys_time',
            'Elapsed (wall clock) time (h:mm:ss or m:ss)': 'real_time',
            'Maximum resident set size (kbytes)': 'max_rss_kb',
            'Exit status': 'exit_code',
        }

        for line in run.temp['time'].open():
            m = re.match('^Command terminated by signal ([0-9]+)$', line)
            if m:
                run.exit_signal = int(m[1])
                continue

            if not line.startswith('\t'):
                continue

            assert ': ' in line, line
            key, val_str = line.strip().split(': ', 1)

            if key not in keymap:
                continue

            val: int | float
            if ':' in val_str:
                secs = 0.0
                for s in val_str.split(':'):
                    secs = secs * 60 + float(s)
                val = secs
            elif '.' in val_str:
                val = float(val_str)
            else:
                val = int(val_str)

            setattr(run, keymap[key], val)

    def extract_benchmark_scores(self, run: Run):
        expected = re.findall(r'''new BenchmarkSuite\(['"]([A-Za-z0-9]+)['"]''', run.test.script)
        if 'BenchmarkSuite.GeometricMeanLatency' in run.test.script and not run.args.v8_v7:
            expected += [s + 'Latency' for s in expected if s in ['Splay', 'Mandreel']]
        if run.test_basename.startswith('richards.'):
            expected += ['Richards']
        run.scores = {s: None for s in expected}

        pattern = r'(%s|Score \(version [0-9]\)): ([0-9.]+(?:e[+-][0-9]+)?)' % ('|'.join(expected))
        for name, score in re.findall(pattern, run.output):
            name = re.sub(r' \(version ([0-9])\)', r'V\1', name)
            if run.args.v8_v7 and name.endswith('Latency'):
                continue
            try:
                score = int(score)
            except:
                try:
                    score = float(score)
                except:
                    continue
                if score == int(score):
                    score = int(score)
                else:
                    score = round(score, 3)
            run.scores[name] = score

    def check_errors(self, run: Run):
        for line in run.output.split('\n'):
            if self.warn_lines_re and re.search(self.warn_lines_re, line):
                continue
            m = re.search(self.error_lines_re, line)
            if m is None:
                continue
            run.errors.append(line)
            break

        if run.exit_signal == 9 and run.max_rss_kb is not None and run.max_rss_kb >= 50 * 1048576:
            run.errors.append('OOM (>%.0fG)' % (run.max_rss_kb / 1048576.))
        elif run.exit_signal is not None:
            try:
                signame = signal.Signals(run.exit_signal).name
                run.errors.append(f'Killed by signal {run.exit_signal} ({signame})')
            except:
                run.errors.append(f'Killed by signal {run.exit_signal}')

        if not run.errors:
            if run.output.strip() == '':
                run.errors.append('No output')
            elif not run.scores:
                run.errors.append('No scores in the output')
            elif run.exit_code != 0:
                run.errors.append(f'Exit code: {run.exit_code}')
            elif run.proc is not None and run.proc.returncode != 0:  # of bash command
                run.errors.append(f'Command "{run.command}" exited with {run.proc.returncode}')

        if self.ignore_errors:
            run.errors = []


class RepSpec:
    """Manages repetition counts and time budgets for benchmark runs."""

    def __init__(self, reps: list[str] | None):
        """Parse -r/--reps arguments into default and per-file specs."""

        self.default_spec = '1'
        self.file_specs: dict[str, str] = {}  # basename -> spec string
        self.counts: dict[str, int] = {}  # path -> runs completed
        self.limits: dict[str, int | None] = {}  # path -> max runs (or None if time-based and not yet calculated)

        if reps:
            for rep_spec in reps:
                if ':' in rep_spec:
                    basename, count = rep_spec.split(':', 1)
                    self.file_specs[basename] = count
                else:
                    self.default_spec = rep_spec

    def should_run(self, path: str) -> bool:
        """Check if we should do another run of this test."""

        basename = os.path.basename(path)

        if basename not in self.counts:
            self.counts[basename] = 0
            self.limits[basename] = None

            spec = self.file_specs.get(basename, self.default_spec)
            if not spec.endswith('s'):
                self.limits[basename] = int(spec)

        limit = self.limits[basename]
        count = self.counts[basename]
        return limit is None or count < limit

    def add(self, run: Run | None, verbose: bool = False):
        """Record that a run completed, update limit if time-based."""

        if not run:
            return

        basename = run.test_basename
        spec = self.file_specs.get(basename, self.default_spec)

        self.counts[basename] += 1

        # Calculate limit for time-based specs after first run
        if spec.endswith('s') and self.limits[basename] is None:
            limit_sec = int(spec[:-1])

            if run.real_time is None or run.real_time < 0.01 or run.real_time > limit_sec:
                self.limits[basename] = 1
            else:
                self.limits[basename] = max(math.floor(limit_sec / run.real_time), 1)

            if verbose:
                reps = self.limits[basename] or 0
                print('real %.2fs => %d rep%s' % (run.real_time or 0, reps, 's' if reps != 1 else ''), flush=True)

    def __getitem__(self, path: str) -> int:
        """Return the number of runs completed for the given path."""

        basename = os.path.basename(path)
        return self.counts.get(basename, 0)


CONFIGS = {
  'besen': Config(
      # too slow on other tests or crashes
      benchmark_suite=['richards.js', 'crypto.js', 'deltablue.js', 'navier-stokes.js'],
      ignore_errors=True,
  ),
  'castl': Config(
      error_lines_re='(?i)(error|unsupported|200 local variables)'
  ),
  'cesanta-v7': Config(
      benchmark_suite=['richards.js'],  # too slow on other tests
  ),
  'dmdscript': Config(
      polyfills=[
          'var console = {log: println}, print = println;',
          ES5_FOR_ES3_PF,
      ],
      filter_lines_re=r'^1 source files$',
  ),
  'dscriptcpp': Config(
      benchmark_suite=['richards.es1.js'],
      polyfills=[
          'var console = {log: println}, print = println;',
          ES5_FOR_ES3_PF,
      ],
  ),
  'echosoar-jsi': Config(
      transforms=[HexTransform()],
  ),
  'espruino': Config(
      filter_lines_re=r'^(%s)$' % '|'.join([r' ____                 _ ', r'\|  __\|___ ___ ___ _ _\|_\|___ ___ ', r'\|  __\|_ -\| . \|  _\| \| \| \|   \| . \|', r'\|____\|___\|  _\|_\| |___\|_\|_\|_\|___\|', r' *\|_\| espruino.com', r'.* (c) 20.. G.Williams', r'Espruino is Open Source. Our work is supported', r'only by sales of official boards and donations:', r'http://espruino.com/Donate']),
  ),
  # -O: expensive optimizations (nearly always helps)
  # -w: disable all warnings
  'hermes': Config(flags=['-O', '-w']),
  'hermes-v1': Config(flags=['-O', '-w']),
  'jerryscript': Config(
      timeout_for_test={
          # Passes splay.js with --mem-heap=65536, but awfully slow: Splay 0.279, SplayLatency 1.50
          # Just time out and don't embrass itself
          'splay.js': 300,
          'typescript.js': 3600,  # passes but very slow
      },
  ),
  'kjs': Config(
      filter_lines_re=r'^LEAK: [0-9]+ KJS::Node$',
  ),
  'lebjs': Config(
      benchmark_suite=['richards.quad-wheel.js'],  # not capable of ES1, no Date()
      polyfills=[],
      timestamp_output=True,
  ),
  'mocha': Config(
      benchmark_suite=['richards.es1.js'],  # JS1.1â‰ˆES1 engine
      polyfills=["var console = new Object(); console.log = print;"],  # no object literals
  ),
  'mquickjs': Config(
      polyfills=[STRICT_PF],
  ),
  'narcissus': Config(
      # prototype-based inheritance not working, need to be fixed
      benchmark_suite=['richards.js1.js'],
  ),
  'nashorn': Config(
      flags=['--language=es6'],
  ),
  'ngs': Config(
      benchmark_suite=['richards.tiny-js.js'],
      polyfills=["var console = {log: print};"],
      timestamp_output=True,
  ),
  'njs': Config(
      # Always runs in strict mode, seemingly no option to disable.
      polyfills=[STRICT_PF],
      transforms=[OctalTransform()],
  ),
  'nova': Config(
      flags=['eval', '--no-strict'],
      warn_lines_re='^Parse errors:$',
      error_lines_re='(^  x |^error:|Error:|TypeError|%s)' % ERROR_LINE_REGEX,
  ),
  'porffor': Config(
      benchmark_suite=['richards.porffor.js'],  # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # print() defined but unusable
  ),
  'quad-wheel': Config(
      benchmark_suite=['richards.quad-wheel.js'],  # not capable of ES1, no Date()
      polyfills=[],
      timestamp_output=True,
  ),
  'quanta': Config(
      benchmark_suite=['richards.quanta.js'],   # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # typeof with undeclared reference throws
  ),
  'quickjs-ng': Config(
      flags=['--script'],  # flaky/buggy autodetect mode
      #TODO retest flags=['--module'], polyfills=[STRICT_PF],
  ),
  'qv4': Config(
      transforms=[OctalTransform()],
      filter_lines_re=r'qt\.qml\.usedbeforedeclared:',
  ),
  'rapidus': Config(
      benchmark_suite=['richards.rapidus.js'],  # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # typeof with undeclared reference throws
      timestamp_output=True                     # unusable Date
  ),
  'sablejs': Config(
      warn_lines_re=r'\[WARN\]',
  ),
  'spidermonkey_1.5': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.6': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.7': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.8.0': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.8.5': Config(flags=['-jm']),  # enable TraceMonkey + method JIT
  'spidermonkey_17': Config(flags=['-m']),      # enable JIT. Enabled by default in 24+.
  'starlight': Config(
      benchmark_suite=['richards.js1.js'],      # not capable of ES1
  ),
  'tiny-js': Config(
      benchmark_suite=['richards.tiny-js.js'],  # not capable of ES1, no Date()
      polyfills=['var console={log: print};'],
      timestamp_output=True,
  ),
  'twostroke': Config(
      benchmark_suite=['richards.es1.js'],
  ),
  'ucode': Config(
      benchmark_suite=['richards.ucode.js'],    # own javascript dialect
      polyfills=[],
  ),
  'yavashark': Config(
      flags=['-i'],
  ),
}

def pick_config(args: argparse.Namespace, engine: Engine) -> Config:
    if args.config and args.config not in CONFIGS:
        sys.exit(f'Unknown config: {args.config}')

    for name in [args.config, engine.path.name, engine.metadata.get('engine')]:
        if name:
            if name in CONFIGS:
                return CONFIGS[name]

            engine_name = name.split('_')[0]
            if engine_name in CONFIGS:
                return CONFIGS[engine_name]

    return Config()


def run_test(engines: list[Engine], test: MemTest, args: argparse.Namespace) -> None:
    def thread_func(engine):
        run = engine.config.benchmark_run(engine, test, args)
        engine.add_run(run)

    if len(engines) == 1:
        thread_func(engines[0])
    else:
        # Run all engines in parallel. Shuffle starting order every time
        engines = random.sample(engines, len(engines))
        for engine in engines:
            engine.current_thread = threading.Thread(target=thread_func, args=(engine,))
            engine.current_thread.start()
        for engine in engines:
            if engine.current_thread is not None:
                engine.current_thread.join()


def write_results(engines: list[Engine]) -> None:
    for engine in engines:
        if engine.output_path and engine.bench_json and engine.bench_json.get('benchmarks'):
            json_str = engine.bench_json_str()
            out_path = engine.output_path
            part_path = out_path.with_suffix(out_path.suffix + '.part')

            with open(part_path, 'w') as fp:
                fp.write(json_str)
                fp.flush()
                fp.close()

            part_path.replace(out_path)


def run_compare(engines: list[Engine], print_command: bool = True) -> None:
    """Run comparison script on output files."""

    bench_dir = Path(os.path.join(os.path.dirname(os.path.abspath(__file__))))
    compare_script = bench_dir / 'compare'
    if not compare_script.exists() or not all(e.output_path for e in engines):
        return

    cmd = [str(compare_script)]
    for e in engines:
        if e.output_path is None:
            continue
        if e.output_path.parent == Path.cwd():
            cmd += [e.output_path.name]
        else:
            cmd += [str(e.output_path)]

    if print_command:
        print('+ %s' % shlex.join(cmd))

    subprocess.run(cmd)


def maybe_pause():
    bench_dir = Path(os.path.join(os.path.dirname(os.path.abspath(__file__))))

    for file in [Path.cwd() / 'pause', bench_dir / 'pause']:
        if file.exists():
            print(f'Pausing while {file.as_posix()} exists')
            while file.exists():
                time.sleep(1)


def main():
    parser = argparse.ArgumentParser(
        usage='%(prog)s [options] engine [engine ...] [--] [test.js ...]',
        description='Benchmark JavaScript engines. Engines and test files are positional arguments. '
                    'Trailing *.js are treated as test scripts (up until optional -- separator). '
                    'To pass flags to an engine, quote the engine with its flags: \'engine flags\'. '
                    'If multiple engines are specified, they will be synchronously tested on '
                    'the same tests in parallel.')
    parser.add_argument('-a', '--append', action='store_true',
                        help='append benchmark runs to existing output file')
    parser.add_argument('-C', '--config', action='append', metavar='engine[_variant]',
                        help='config to use')
    parser.add_argument('-k', '--keep', action='store_true',
                        help='keep temp dir')
    parser.add_argument('-o', '--output', action='append', metavar='file.json',
                        help='output json file for benchmark results')
    parser.add_argument('-r', '--reps', action='append', type=str, metavar='[basename:]count[s]',
                        help='repeat each benchmark this many times. Can specify as a time budget '
                             'and per-test limits: -r 60s -r zlib.js:10 -> run zlib.js 10 times, '
                             'others for 1 minute.')
    parser.add_argument('-t', '--timeout', type=float, metavar='seconds',
                        help='time limit for each single test execution')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='verbose execution')
    parser.add_argument('-7', '--v8-v7', action='store_true',
                        help='run on v8-v7 test suite')
    parser.add_argument('--skip-unchanged', action='store_true',
                        help="skip if output file exists with same binary's revision")

    args, remaining = parser.parse_known_args()

    # Parse engines and test files from remaining positional arguments
    if '--' in remaining:
        i = remaining.index('--')
        args.engines = remaining[:i]
        args.tests = remaining[i + 1:]
    else:
        # Pop trailing .js files as tests, but keep at least one arg as engine
        args.tests = []
        while len(remaining) > 1 and remaining[-1].endswith('.js'):
            args.tests.append(remaining.pop())
        args.tests.reverse()
        args.engines = remaining

    if not args.engines:
        parser.error('At least one engine must be specified')

    engines = [Engine(spec) for spec in args.engines]

    # Set/choose config
    if args.config:
        if len(args.config) != len(engines):
            sys.exit(f'Error: number of -C flags must match number of engines')
        for idx, engine in enumerate(engines):
            config_name = args.config[idx]
            if config_name not in CONFIGS:
                sys.exit(f'Unknown config: {config_name}')
            engine.config = CONFIGS[config_name]
    else:
        for engine in engines:
            engine.config = pick_config(args, engine)

    # Set/generate output paths
    if args.output:
        if len(args.output) != len(engines):
            sys.exit(f'Error: number of -o flags must match number of engines')
        for i, engine in enumerate(engines):
            engine.output_path = Path(args.output[i])
    else:
        next_suffix = {}
        for engine in engines:
            base_name = engine.path.name
            engine.output_path = Path.cwd() / f'{base_name}{next_suffix.get(base_name, "")}.bench'
            next_suffix[base_name] = next_suffix.get(base_name, 1) + 1
            engine.output_path.parent.mkdir(exist_ok=True)
            if args.verbose:
                print(f'Output file: {str(engine.output_path)}')

    # Read existing output files for --skip-unchanged and --append
    for engine in engines:
        if engine.output_path and engine.output_path.exists():
            try:
                prev_bench = json.loads(engine.output_path.open().read())
            except json.JSONDecodeError:
                prev_bench = None

            if args.skip_unchanged and prev_bench and len(engines) == 1:
                if engine.metadata.get('revision'):
                    if engine.metadata['revision'] == prev_bench.get('metadata', {}).get('revision'):
                        print(f'Skipping {engine.path}: same revision as previous run')
                        return
                elif engine.metadata.get('revision_date'):
                    if engine.metadata['revision_date'] == prev_bench.get('metadata', {}).get('revision_date'):
                        print(f'Skipping {engine.path}: same revision date as previous run')
                        return

            if args.append and prev_bench:
                engine.bench_json = prev_bench

    # Determine test files
    if not args.tests:
        if engines[0].config.benchmark_suite:
            args.tests = engines[0].config.benchmark_suite
        elif args.v8_v7:
            args.tests = V8_V7_TESTS
        else:
            args.tests = OCTANE_TESTS
    assert args.tests

    bench_dir = Path(os.path.join(os.path.dirname(os.path.abspath(__file__))))
    last_write_time = time.time()
    reps = RepSpec(args.reps)
    need_final_compare = False

    # Run tests in order
    try:
        for filename in args.tests:
            path = bench_dir / filename
            test = load_test(path)

            while reps.should_run(filename):
                maybe_pause()
                run_test(engines, test, args)
                reps.add(engines[0].runs[-1], args.verbose)

                if any(e.runs[-1].errors for e in engines):
                    break

                # Write partial results periodically
                if time.time() - last_write_time >= PERIODIC_SAVE_SECONDS:
                    write_results(engines)
                    last_write_time = time.time()

            # Write partial results after each benchmark finishes
            write_results(engines)
            last_write_time = time.time()

            if reps[filename] > 1 and len(engines) > 1:
                run_compare(engines)
                need_final_compare = False
            else:
                need_final_compare = True

    except KeyboardInterrupt:
        print(f'Aborting benchmarking')
        for engine in engines:
            engine.kill()
        sys.exit(1)

    write_results(engines)
    if need_final_compare:
        run_compare(engines)


if __name__ == '__main__':
    main()
