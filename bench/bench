#!/usr/bin/env python3
# Basic usage:
#   ./bench.py binary [test.js ...]
#
# Conceptually, runs './binary test.js' on each test multiple times,
# manages quirks of different engines, default set of benchmarks,
# parses errors and test results.
#
# SPDX-FileCopyrightText: 2025 Ivan Krasilnikov
# SPDX-License-Identifier: MIT

import argparse
import glob
import json
import math
import os
import platform
import re
import shlex
import shutil
import signal
import subprocess
import sys
import tempfile
import time

from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union

OCTANE_TESTS = [
   'richards.js',
   'deltablue.js',
   'crypto.js',
   'raytrace.js',
   'earley-boyer.js',
   'regexp.js',
   'splay.js',
   'navier-stokes.js',
   'pdfjs.js',
   'mandreel.js',
   'gbemu.js',
   'code-load.js',
   'box2d.js',
   'zlib.js',
   'typescript.js',
]

PRINT_PF = '''
  if (typeof print == "undefined" && typeof "console" != "undefined") {
    if (typeof globalThis != "undefined") globalThis.print = console.log;
    else print = console.log;
  }
  if (typeof console == "undefined") {
    if (typeof globalThis != "undefined") globalThis.console = new Object();
    else console = new Object();
  }
  if (typeof console.log == "undefined" && typeof print != "undefined") {
    console.log = print;
  }
'''

ES5_FOR_ES3_PF = '''
  if (typeof Array.prototype.indexOf === "undefined") {
    Array.prototype.indexOf = function(x, i) {
      for (i = i || 0; i < this.length; i++) if (this[i] === x) return i;
      return -1;
    };
  }
  if (typeof Array.prototype.map === "undefined") {
    Array.prototype.map = function(f, t) {
      for (var i = 0, res = []; i < this.length; i++)
        if (i in this) res[i] = f.call(t, this[i], i, this);
      return res;
    };
  }
  if (typeof Object.create === "undefined") {
    Object.create = function(p) { function F() {}; F.prototype = p; return new F(); };
  }
  if (typeof Object.defineProperty === "undefined") {
    Object.defineProperty = function(obj, prop, desc) {
      if (desc.hasOwnProperty('value')) {
        obj[prop] = desc.value;
      } else if (desc.hasOwnProperty('get')) {
        obj[prop] = desc.get.call(obj);
      }
      return obj;
    };
  }
'''

# For engines that run in strict mode by default
STRICT_PF = '''
  var alert = print;
  var setupEngine, nValue, eValue, dValue, pValue, qValue, dmp1Value, dmq1Value, coeffValue;  // crypto.js
  var result;  // earley-boyer.js
  var Mandreel_timeouts, Mandreel_XMLHttpRequest, Mandreel_document, Mandreel_window;  // mandreel.js
'''

@dataclass
class MemTest:
    """In-memory test case, a single self-contained script."""

    basename: str
    script: str

def load_test(path: Union[str, Path], drop_loads=False) -> MemTest:
    """Read test case into memory, recursively resolving load() calls."""

    path = Path(path)
    basename = path.name
    lines = []

    for line in open(path):
        m = re.match(r'''^load\((?:base_dir \+ )?('[^']+'|"[^"\\]+")\) *;$''', line.strip());
        if not m:
            lines.append(line)
            continue

        lines.append('// BEGIN ' + line.strip() + '\n')
        if drop_loads:
            continue

        loaded = load_test(path.parent / m[1][1:-1])
        lines.append(loaded.script.strip() + '\n')
        lines.append('// END ' + line.strip() + '\n\n')

    script = ''.join(lines).strip() + '\n'
    return MemTest(basename=basename, script=script)

TestTransform = Callable[[MemTest], Union[str, MemTest]]

class OctaneHarness:
    def __init__(self):
        self._base = None
        self._tail = None

    def __call__(self, test: MemTest) -> MemTest:
        if test.basename not in OCTANE_TESTS:
            return test

        if 'function Benchmark(' in test.script:
            return test

        if self._base is None:
            self._base = open('octane/base.js').read().strip() + '\n'

        if self._tail is None:
            self._tail = open('octane/run.js').read().strip() + '\n'
            self._tail = re.sub(r'load\([^)]+\);', '', self._tail)

        script = self._base + test.script + self._tail
        return MemTest(basename=test.basename, script=script)

class OctalTransform:
    """Fix octal char sequences in earley-boyer.js, "\000" -> "\x00" etc."""

    def __call__(self, test: MemTest) -> str:
        return re.sub(r'"\\[01]([0-9][0-9])"', self._replace, test.script)

    def _replace(self, match: re.Match) -> str:
        return '"\\x%.2X"' % ord(eval(match[0])[0])

class HexTransform:
    """Replace hex literals by decimal."""

    def __call__(self, test: MemTest) -> str:
        return re.sub(r'''(0x[0-9a-fA-F]+|"(?:[^"\n]|\")+"|'(?:'[^'\n]|\')+')''', self._replace, test.script)

    def _replace(self, match: re.Match) -> str:
        if match[0].startswith('0x'):
            return str(eval(match[0]))
        return match[0]

#lambda test: re.sub(r'performance.now = \(function\(\) {[^}]+}\)\(\);',
#                    '', test.script, re.MULTILINE),

class PolyfillTransform:
    def __init__(self, snippets: List[str]):
        lines = []
        for snippet in snippets:
            for line in snippet.strip().split('\n'):
                lines.append(line.strip())
        self.code = '\n'.join(lines) + '\n'

    def __call__(self, test: MemTest) -> str:
        return self.code + test.script

class Binary:
    path: Path
    flags: Optional[List[str]]
    metadata: Dict[str, Any]

    def __init__(self, path_and_flags: str):
        self.path = Path(path_and_flags)
        self.flags = None
        self.metadata = {}

        if ' ' in path_and_flags and not self.path.exists():
            args = shlex.split(path_and_flags)
            assert len(args) >= 2
            self.path = Path(args[0])
            self.flags = args[1:]

        # allow benchmarking binaries in PATH, e.g. node
        if not os.path.exists(self.path) and '/' not in str(self.path):
            which_path = shutil.which(self.path)
            if which_path:
                self.path = Path(which_path)

        if not os.path.exists(self.path):
            raise FileNotFoundError(self.path)

        self.path = Path(os.path.abspath(str(self.path)))

        json_path = self.path.parent / f'{self.path.name}.json'
        if json_path.exists():
            self.metadata = json.load(json_path.open())

    def __str__(self):
        return f'Binary({repr(self.__dict__)})'

@dataclass
class Run:
    temp: Dict[str, Path]  # Temp paths
    binary_path: Path
    flags: List[str]
    metadata: Dict[str, Any]
    test: MemTest
    test_basename: str
    args: argparse.Namespace
    timeout: Optional[float] = None
    command: str = ''  # bash command
    output: str = ''   # stdout+stderr combined
    errors: List[str] = field(default_factory=list)
    # Result of subprocess.run() with self.command
    proc: Optional[subprocess.CompletedProcess] = None
    # exit_code..max_rss_kb: as measured by /usr/bin/time
    # Note: exit_code captures exit code of javascript shell itself,
    # vs proc.returncode - exit code of bash command/pipeline
    exit_code: Optional[int] = None
    exit_signal: Optional[int] = None
    user_time: Optional[float] = None  # seconds
    real_time: Optional[float] = None
    sys_time: Optional[float] = None
    max_rss_kb: Optional[int] = None
    scores: Dict[str, Union[int, float]] = field(default_factory=dict)

    def to_dict(self):
        res = {}
        for k, v in self.__dict__.items():
            if k in ['proc', 'args', 'temp']:
                continue
            elif hasattr(v, 'to_dict'):
                res[k] = v.to_dict()
            else:
                res[k] = v
        return res

ERROR_LINE_REGEX_I = "(?i)(?:error|exception|uncaught|mismatch|failed|invalid|incorrect|unsupported|cannot|can't)"
ERROR_LINE_REGEX = "(?:error|exception|uncaught|mismatch|failed|invalid|incorrect|unsupported|cannot|can't)"

DEFAULT_TIMEOUT = 1800

class Config:
    # Flags to pass to binary (before script path). binary.flags takes precedence.
    flags: List[str]
    transforms: List[TestTransform]
    polyfills: List[str]
    # Pattern for error lines
    error_lines_re: re.Pattern
    # Pattern for mere warning lines, takes precedence over error_lines_re
    warn_lines_re: Optional[re.Pattern]
    # Pattern for lines to immediately filter out during the run, via egrep
    filter_lines_re: Optional[str]
    timestamp_output: bool
    timeout: float
    # test basename => timeout
    timeout_for_test: Dict[str, float]
    ignore_errors: bool
    benchmark_suite: Optional[List[str]]

    def __init__(
            self,
            flags: List[str] = [],
            polyfills: List[str] = [PRINT_PF],
            transforms: List[TestTransform] = [],
            error_lines_re: str = ERROR_LINE_REGEX_I,
            warn_lines_re: Optional[str] = None,
            filter_lines_re: Optional[str] = None,
            timestamp_output: bool = False,
            timeout: float = DEFAULT_TIMEOUT,
            timeout_for_test: Dict[str, float] = {},
            ignore_errors: bool = False,
            benchmark_suite: Optional[List[str]] = None,
        ):
        self.flags = list(flags)
        self.transforms = list(transforms)
        if polyfills:
            self.transforms.append(PolyfillTransform(polyfills))
        self.error_lines_re = re.compile(error_lines_re)
        self.warn_lines_re = re.compile(warn_lines_re) if warn_lines_re else None
        self.filter_lines_re = filter_lines_re
        self.timestamp_output = timestamp_output
        self.timeout = timeout
        self.timeout_for_test = timeout_for_test
        self.ignore_errors = ignore_errors
        self.benchmark_suite = benchmark_suite

    def benchmark_run(self, binary: Binary, test: MemTest, args: argparse.Namespace) -> Run:
        temp_dir=Path(tempfile.mkdtemp(prefix=f'{binary.path.name}-{test.basename.removesuffix(".js")}-'))

        run = Run(
            temp = {
                'dir': temp_dir,
                'output': temp_dir / 'output',
                'time': temp_dir / 'time',
                'script': temp_dir / test.basename,
            },
            binary_path=binary.path,
            flags=(self.flags if binary.flags is None else binary.flags),
            metadata=binary.metadata,
            test=test,
            test_basename=test.basename,
            args=args,
        )

        if run.args.timeout:
            run.timeout = run.args.timeout
        else:
            run.timeout = self.timeout_for_test.get(
                os.path.basename(run.test_basename), self.timeout)

        self.transform_test(run)
        self.save_script(run)
        self.build_command(run)
        self.run_command(run)

        if run.temp['output'].exists():
            run.output = run.temp['output'].open().read()

        self.parse_time_output(run)
        self.extract_benchmark_scores(run)
        self.check_errors(run)

        if run.errors or run.args.keep:
            with open(temp_dir / 'run.json', 'w') as fp:
                json.dump(run.__dict__, indent=2, fp=fp, default=repr)
        else:
            shutil.rmtree(temp_dir)

        return run

    def transform_test(self, run: Run):
        test = run.test
        for f in self.transforms:
            res = f(test)
            if type(res) is MemTest:
                test = res
            else:
                assert type(res) is str
                test = MemTest(basename=test.basename, script=res)
        run.test = test

    def save_script(self, run: Run):
        with open(run.temp['script'], 'w') as fp:
            fp.write(run.test.script)

    def build_command(self, run: Run):
        run.command = shlex.join(['cd', run.temp['dir'].as_posix()])
        run.command += '; ' + shlex.join(
            ['stdbuf', '-oL', '-eL'] +
            ['/usr/bin/time', '-v', '-o', 'time'] +
            [run.binary_path.as_posix()] +
            run.flags +
            [run.temp['script'].name]
        ) + ' 2>&1'
        if self.timestamp_output:
            # Prefix output lines with time relative to start, n.nnnnnn
            run.command += ' | ts -s %.s'
        if self.filter_lines_re:
            run.command += ' | ' + shlex.join(['egrep', '-v', '-e', self.filter_lines_re])
        run.command += ' | tee output'

    def run_command(self, run: Run):
        if not run.args.quiet:
            print(f'> {run.command}', flush=True)
        try:
            run.proc = subprocess.Popen(
                ['/bin/bash', '-e', '-o', 'pipefail', '-c', run.command],
                start_new_session=True,
            )
            run.proc.wait(timeout=run.timeout)
        except subprocess.TimeoutExpired:
            run.errors.append('Timeout (>%.0fs)' % run.timeout)
        finally:
            # Kill whole process group, not just directly launched process
            if run.proc.poll() is None:
                os.killpg(os.getpgid(run.proc.pid), signal.SIGKILL)
                run.proc.wait()

    def parse_time_output(self, run: Run):
        """Parse the output of /usr/bin/time -v"""

        assert run.temp['time'].exists()

        keymap = {
            'User time (seconds)': 'user_time',
            'System time (seconds)': 'sys_time',
            'Elapsed (wall clock) time (h:mm:ss or m:ss)': 'real_time',
            'Maximum resident set size (kbytes)': 'max_rss_kb',
            'Exit status': 'exit_code',
        }

        for line in run.temp['time'].open():
            m = re.match('^Command terminated by signal ([0-9]+)$', line)
            if m:
                run.exit_signal = int(m[1])
                continue

            if not line.startswith('\t'):
                continue

            assert ': ' in line, line
            key, val = line.strip().split(': ', 1)

            if key not in keymap:
                continue

            if ':' in val:
                secs = 0
                for s in val.split(':'):
                    secs = secs * 60 + float(s)
                val = secs
            elif '.' in val:
                val = float(val)
            else:
                val = int(val)

            setattr(run, keymap[key], val)

    def extract_benchmark_scores(self, run):
        expected = re.findall(r'''new BenchmarkSuite\(['"]([A-Za-z0-9]+)['"]''', run.test.script)
        if 'BenchmarkSuite.GeometricMeanLatency' in run.test.script:
            expected += [s + 'Latency' for s in expected if s in ['Splay', 'Mandreel']]
        if run.test_basename.startswith('richards.'):
            expected += ['Richards']
        run.scores = {s: None for s in expected}

        pattern = r'(%s|Score \(version [0-9]\)): ([0-9.]+(?:e[+-][0-9]+)?)' % ('|'.join(expected))
        for name, score in re.findall(pattern, run.output):
            name = re.sub(r' \(version ([0-9])\)', r'V\1', name)
            try:
                score = int(score)
            except:
                try:
                    score = float(score)
                except:
                    continue
                if score == int(score):
                    score = int(score)
                else:
                    score = round(score, 3)
            run.scores[name] = score

    def check_errors(self, run: Run):
        for line in run.output.split('\n'):
            if self.warn_lines_re and re.search(self.warn_lines_re, line):
                continue
            m = re.search(self.error_lines_re, line)
            if m is None:
                continue
            run.errors.append(line)
            break

        if run.exit_signal == 9 and run.max_rss_kb >= 50 * 1048576:
            run.errors.append('OOM (>%.0fG)' % (run.max_rss_kb / 1048576.))
        elif run.exit_signal is not None:
            try:
                signame = signal.Signals(run.exit_signal).name
                run.errors.append(f'Killed by signal {run.exit_signal} ({signame})')
            except:
                run.errors.append(f'Killed by signal {run.exit_signal}')

        if not run.errors:
            if run.output.strip() == '':
                run.errors.append('No output')
            elif not run.scores:
                run.errors.append('No scores in the output')
            elif run.exit_code != 0:
                run.errors.append(f'Exit code: {run.exit_code}')
            elif run.proc.returncode != 0:  # of bash command
                run.errors.append(f'Command "{run.command}" exited with {run.proc.returncode}')

        if self.ignore_errors:
            run.errors = []

def coalesce_runs_to_json(runs: List[Run]) -> str:
    """Produce a single summary json (returned pre-formatted as string)
       from all benchmark runs for a single binary."""

    res = {
        'binary': runs[-1].binary_path.name,
        'flags': runs[-1].flags,
        'metadata': runs[-1].metadata,
        'benchmarks': {},
    }

    if not res['flags']:
        res.pop('flags')

    for run in runs:
        for key in run.scores.keys():
            res['benchmarks'][key] = {'score': [], 'error': '', 'user': [], 'sys': [], 'real': [], 'rss_mb': []}

    for run in runs:
        for key in run.scores.keys():
            d = res['benchmarks'][key]
            if run.errors:
                d['error'] = run.errors[0]
            if run.scores[key] is not None:
                d['score'].append(run.scores[key])
            if run.user_time is not None:
                d['user'].append(run.user_time)
            if run.sys_time is not None:
                d['sys'].append(run.sys_time)
            if run.real_time is not None:
                d['real'].append(run.real_time)
            if run.max_rss_kb is not None:
                d['rss_mb'].append(round(run.max_rss_kb / 1024.0, 2))

    for d in res['benchmarks'].values():
        if d['error']:
            del d['score']
        else:
            del d['error']
        for k in ['user', 'sys', 'real', 'rss_mb']:
            if d[k] is not None and len(d[k]) == 0:
                del d[k]

    # indent except arrays
    s = json.dumps(res, indent=2)
    s = re.sub(r'(?<=": )(\[[^\[\]]+\])', lambda m: json.dumps(json.loads(m[1])), s)
    return s

# TODO json/jsonnet config
CONFIGS = {
  'besen': Config(
      # too slow on other tests or crashes
      benchmark_suite=['richards.js', 'crypto.js', 'deltablue.js', 'navier-stokes.js'],
      ignore_errors=True,
  ),
  'castl': Config(
      error_lines_re='(?i)(error|unsupported|200 local variables)'
  ),
  'cesanta-v7': Config(
      benchmark_suite=['richards.js'],  # too slow on other tests
  ),
  'dmdscript': Config(
      polyfills=[
          'var console = {log: println}, print = println;',
          ES5_FOR_ES3_PF,
      ],
  ),
  'dscriptcpp': Config(
      benchmark_suite=['richards.es1.js'],
      polyfills=[
          'var console = {log: println}, print = println;',
          ES5_FOR_ES3_PF,
      ],
  ),
  'echosoar-jsi': Config(
      transforms=[HexTransform()],
  ),
  'hermes': Config(
      flags=['-O'],
      warn_lines_re=r"(warning:|error\('Incorrect|throw 'Unsupported)",
  ),
  'jerryscript': Config(
      timeout_for_test={
          # Passes splay.js with --mem-heap=65536, but awfully slow: Splay 0.279, SplayLatency 1.50
          # Just time out and don't embrass itself
          'splay.js': 300,
          'typescript.js': 3600,  # passes but very slow
      },
  ),
  # TODO 'js-interpreter_jsc': Config(host_engine='jsc'),  --want to configure to run it with custom shell
  'mocha': Config(
      benchmark_suite=['richards.es1.js'],  # JS1.1â‰ˆES1 engine
      polyfills=["var console = new Object(); console.log = print;"],  # no object literals
  ),
  'narcissus': Config(
      # prototype-based inheritance not working, need to be fixed
      benchmark_suite=['richards.js1.js'],
  ),
  'nashorn': Config(
      flags=['--language=es6'],
  ),
  'nashorn_ot': Config(
      flags=['--language=es6', '-ot'],
  ),
  'njs': Config(
      # Always runs in strict mode, seemingly no option to disable.
      polyfills=[STRICT_PF],
      transforms=[OctalTransform()],
  ),
  'nova': Config(
      flags=['eval', '--no-strict'],
      warn_lines_re='^Parse errors:$',
      error_lines_re='(^  x |^error:|Error:|TypeError|%s)' % ERROR_LINE_REGEX,
  ),
  'porffor': Config(
      benchmark_suite=['richards.porffor.js'],  # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # print() defined but unusable
  ),
  'quad-wheel': Config(
      benchmark_suite=['richards.quad-wheel.js'],  # not capable of ES1, no Date()
      polyfills=[],
      timestamp_output=True,
  ),
  'quanta': Config(
      benchmark_suite=['richards.quanta.js'],   # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # typeof with undeclared reference throws
  ),
  'quickjs-ng': Config(
      flags=['--script'],  # flaky/buggy autodetect mode
      #TODO retest flags=['--module'], polyfills=[STRICT_PF],
  ),
  'qv4': Config(
      transforms=[OctalTransform()],
      filter_lines_re=r'qt\.qml\.usedbeforedeclared:',
  ),
  'rapidus': Config(
      benchmark_suite=['richards.rapidus.js'],  # can't deal with standard test harness, various bugs/limitations
      polyfills=['var print=console.log;'],     # typeof with undeclared reference throws
      timestamp_output=True                     # unusable Date
  ),
  'sablejs': Config(
      warn_lines_re=r'\[WARN\]',
  ),
  # TODO benchmark_polyfills=[]
  'spidermonkey_1.5': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.6': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.7': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.8.0': Config(polyfills=[PRINT_PF, ES5_FOR_ES3_PF]),
  'spidermonkey_1.8.5': Config(flags=['-jm']),  # enable TraceMonkey + method JIT
  'spidermonkey_17': Config(flags=['-m']),      # enable JIT. Enabled by default in 24+.
  'starlight': Config(
      benchmark_suite=['richards.js1.js'],      # not capable of ES1
  ),
  'tiny-js': Config(
      benchmark_suite=['richards.tiny-js.js'],  # not capable of ES1, no Date()
      polyfills=['var console={log: print};'],
      timestamp_output=True,
  ),
  'ucode': Config(
      benchmark_suite=['richards.ucode.js'],    # own javascript dialect
      polyfills=[],
  ),
}

def pick_config(args, binary):
    if args.config and args.config not in CONFIGS:
        sys.exit(f'Unknown config: {args.config}')

    for name in [args.config, binary.path.name, binary.metadata.get('engine')]:
        if name:
            if name in CONFIGS:
                return CONFIGS[name]

            engine = name.split('_')[0]
            if engine in CONFIGS:
                return CONFIGS[engine]

    return Config()

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-c', '--config', help='Config to use, "engine[_variant]"')
    parser.add_argument('-k', '--keep', action='store_true', help='Keep temp dir')
    parser.add_argument('-o', '--output', help='Write benchmark results here as json')
    parser.add_argument('-q', '--quiet', action='store_true')
    parser.add_argument('-r', '--reps', type=int, metavar='repetitions')
    parser.add_argument('-t', '--timeout', type=float, metavar='seconds')
    parser.add_argument('binary', help='Binary to execute, optionally with flags')
    parser.add_argument('tests', nargs='*', help='Test files to run')
    args = parser.parse_args()

    binary = Binary(args.binary)

    config = pick_config(args, binary)
    if args.tests:
        test_files = args.tests
    elif config.benchmark_suite:
        test_files = config.benchmark_suite
    else:
        test_files = OCTANE_TESTS
    assert test_files

    bench_dir = Path(os.path.join(os.path.dirname(os.path.abspath(__file__))))

    output_path = args.output
    if not (output_path or args.tests or args.config or binary.flags):
        output_path = bench_dir / (binary.path.name + '.json')
        output_path.parent.mkdir(exist_ok=True)
        print('Saving results to: %s' % str(output_path))
    if output_path:
        output_path = Path(output_path)

    all_runs = []

    # TODO: run reps in zigzag order

    for filename in test_files:
        path = bench_dir / filename
        test = load_test(path)
        test_runs = []
        limit = args.reps

        if limit is None and 'features/' in path.as_posix():
            limit = 1

        while limit is None or len(test_runs) < limit:
            run = config.benchmark_run(binary, test, args)

            test_runs.append(run)
            all_runs.append(run)

            if run.errors:
                break

            if limit is None:
                if run.real_time is None or run.real_time > 300:
                    limit = 1
                else:
                    limit = math.ceil(300 / run.real_time)
                if not args.quiet:
                    print('real %.2fs => %d reps' % (run.real_time or 0, limit), flush=True)

        json_str = coalesce_runs_to_json(all_runs)
        if output_path:
            with open(output_path.with_suffix('.json.part'), 'w') as fp:
                fp.write(json_str)

    json_str = coalesce_runs_to_json(all_runs)
    if not args.quiet and not '"benchmarks": {}' in json_str:
        print(json_str)

    if output_path:
        with open(output_path, 'w') as fp:
            fp.write(json_str)
        output_path.with_suffix('.json.part').unlink(missing_ok=True)
        print('Saved results to: %s' % str(output_path))

if __name__ == '__main__':
    main()
